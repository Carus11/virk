####################################################################
#### viya_pre_install_playbook.yml                              ####
####################################################################
#### Author: SAS Institute Inc.                                 ####
####                                                            ####
####  WARNING: this playbook only works                         ####
####           with Ansible 2.2 and above.                      ####
####                                                            ####
####################################################################


####################################################################
#
# This playbook verifies and possibly performs many of the
# pre-requisites for a generic Viya deployment.
#
# to see an index of the things it does, run:
#
#        ansible-playbook viya_pre_install_playbook.yml --list-tasks
#
# to see how far from the desired state you are, run:
#
#        ansible-playbook viya_pre_install_playbook.yml --check
#
# to let the playbook make all the required changes and bring you
# to the desired state, run:
#
#        ansible-playbook viya_pre_install_playbook.yml
#
# useful tags to know:
#
#  - skipifbelowspecs: allows the playbook to run even if your
#    servers do not meet the specs (CPU, Mem, Storage)
#
#         If you server(s) fails one of the specs tests, the error
#         message will tell you how to bypass that check.
#
#  - detectableonly: tasks where we can detect the wrong or right
#    config, but can't fix it
#
#  - fixable:  tasks where we can both
#
####################################################################


---
- hosts: all
  become: yes
  become_user: root

  vars:

    ## OS names and version
    redhat_os_name: RedHat
    os_major_version_1: 7
    os_major_minor_version_1: 7.1
    os_major_version_2: 6
    os_major_minor_version_2: 6.7

    ## local users ##
    create_local_cas_and_sas_accounts: true
    cas_user: cas
    sas_user: sas
    sas_group: sas
    umask_value: "0002"
    custom_group_list:
      - { group: "{{sas_group}}",       gid: 1001 }

    custom_user_list:
      - { name: "{{sas_user}}" , uid: 1001 ,  group: "{{sas_group}}",  groups: "{{sas_group}}"}
      - { name: "{{cas_user}}" , uid: 1002 ,  group: "{{sas_group}}",  groups: "{{sas_group}}"}

    ## storage ##
    storage_list:
      # update as needed.
      - { path: /, min_storage_mb: 5000, min_io: 2}
      - { path: /opt/sas, min_storage_mb: 40000, min_io: 2}

      #
      # this line can be added to check on another partition
      #
      #- { path: /sastmp, min_storage_mb: 4000, min_io: 99}
      #

    ## Minimum total Memory on each machine
    min_mem_mb: 40000

    ## Minimum number of cores on each machine
    min_cores_num: 4

    ## SSH Max Startups
    maxstartups_val: 100

    ## ulimit values
    ulimit_nofile_val: 20480
    ulimit_soft_nproc_val: 65536

    # max_hostname_length: 64       ## official number from the doc
    max_hostname_length: 58        ## better number, SAS note is yet to be written.

    ## version-specific and solution-specific
    viya_version: 3.3

    ## yum related
    yum_cache_yn: 0                ## do you want to turn the yum cache on or off? (1=on, 0=off)
    yum_cache_min_space_mb: 8000   ## how many MB of free space should be in /var/yum/cache ?

    ## general packages that are mandatory for Viya to work

    yum_packages_general:
      - java
      - numactl
      - libXp.x86_64
      - libXp.i686
      - libXext
      - libXmu
      - libXtst
      - xterm             ## this is required for the xterm functions from CAS Monitor
      - xorg-x11-xauth
      - strace            ## may be neeeded for hadooptracer
      - net-tools         ## you need to have netstat on the server

    yum_packages_nicetohave:
      - firefox           ## a browser on the server can be useful
      - xclock            ## can be used to test X11 display
      - mlocate           ## quicker way to find your files
      - wget
      #- python-passlib   ## useful to encrypt passwords with python
      - curl
      - openldap-clients  ## very useful for testing AD/LDAP connection
      #- sshpass          ## sshpass can be problematic so ignore_errors has been added
      - python-firewall
      - lsof
      - tmux              ## help insure against accidental disconnections of SSH
      - ntp               ## ntp helps ensure that servers have synchronized time

    yum_packages_rhel6:
      - libpng

    yum_packages_rhel7:
      - libpng12

    ##
    ## SAS YUM repos.
    ##
    ## if your servers can not reach them, you are going to have to create a local YUM mirror
    ##
    sas_yum_urls:
      - ses.sas.download
      - bwp1.ses.sas.download
      - bwp2.ses.sas.download
      - sesbw.sas.download

    ## maximum number of network interfaces (excluding loopback)
    max_num_nics: 1

    ## third-party software requirements
    required_ansible_min_version: "2.2"
    required_python_min_version: "2.6"

    ## do a 60 seconds pause at the begginning if not using the --check option
    use_pause: true

    ## not used.
    #admin_email: me@somewhere.com


  tasks:

  - name: "WARNING: If you are running this playbook without the --check parameter, it WILL perform changes to your systems."
    pause:
      seconds: 60
      prompt: "Press 'ctl+c' and then 'A' to abort the process. The playbook will continue after 60 seconds"
    when: (ansible_check_mode == False) and (use_pause == True)
    tags:
      - always

####################################################################
## Ping
####################################################################
  - name: Ping all the servers
    ping:
    tags:
      - ping

####################################################################
## OS Version Check
####################################################################
# Test harness:
## fail because not ubuntu
##    ansible-playbook viya_pre_install_playbook.yml --tags os_version_check -e "use_pause=0" -e "redhat_os_name=ubuntu"
## fail because neither 4 or 5
##    ansible-playbook viya_pre_install_playbook.yml --tags os_version_check -e "use_pause=0" -e "os_major_version_1=4 os_major_version_2=5"
## fail because not high enough within 6 or 7.
##    ansible-playbook viya_pre_install_playbook.yml --tags os_version_check -e "use_pause=0" -e "os_major_version_1=6 os_major_version_2=7 os_major_minor_version_1=6.99 os_major_minor_version_2=7.99"

  - name: Ensure that the Operating System is supported for Viya
    assert:
      that:
        # the OS is redhat
        - (ansible_os_family == redhat_os_name )
        # either 6 or 7
        - ansible_distribution_major_version | version_compare(os_major_version_1, '==')
          or
          ansible_distribution_major_version | version_compare(os_major_version_2, '==')
        # at least 6.7 if 6 or at least 7.1 if 7
        - (ansible_distribution_major_version | version_compare(os_major_version_1, '==')
          and
          ansible_distribution_version | version_compare(os_major_minor_version_1, '>='))
          or
          (ansible_distribution_major_version | version_compare(os_major_version_2, '==')
          and
          ansible_distribution_version | version_compare(os_major_minor_version_2, '>='))
      msg:
        - "The OS of this server is not supported."
        - "Viya requires {{redhat_os_name}} {{os_major_version_1}} or {{os_major_version_2}} "
        - "(at least {{os_major_minor_version_1}} or {{os_major_minor_version_2}} )"
        - "You seem to have {{ansible_os_family}} {{ansible_distribution_version}}"
    tags:
      - os_version_check
      - os_name_check

####################################################################
## Third-party Software Check
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags third_party_check -e use_pause=0
#   make it fail
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags third_party_check -e use_pause=0 -e required_ansible_min_version=2.6
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags third_party_check -e use_pause=0 -e required_python_min_version=2.9
#

  - block:
    ## third-party checks block
    - name: Check for minimum version of Ansible
      fail: msg="It looks like your are using Ansible version {{ansible_version.full}} but SAS deployment requires version {{required_ansible_min_version}}."
      when: ansible_version.full | version_compare(required_ansible_min_version, '<')
      tags:
        - ansible_version_check

    - name: Check for minimum version of Python
      fail: msg="It looks like your are using Python {{ansible_python_version}} but SAS deployment requires Python {{required_python_min_version}}."
      when: ansible_python_version | version_compare(required_python_min_version, '<')
      tags:
        - python_version_check

    ## end of third-party block
    tags:
      - third_party_check

####################################################################
## Memory Check
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml -e "use_pause=0" --tags memory_check -e "min_mem_mb=1"
#   make it fail
#     ansible-playbook viya_pre_install_playbook.yml --tags memory_check -e "use_pause=0" -e "min_mem_mb=9999999"

  - block:
    ## Display all the values:
    - name: Query the Server Memory for all servers being used in deployment
      setup:
        filter: "*memory*"
      register: memory

    - name: Show all memory info gathered by Ansible
      debug: var=memory

    - name: Show total memory info
      debug: msg="Total Memory is {{ ansible_memory_mb.real.total }} MB"
    - name: Show free memory info
      debug: msg="Free  Memory is {{ ansible_memory_mb.real.free }} MB"

    - name: Assert that there is enough ({{(min_mem_mb|int)/1000}} GB) memory on the server
      assert:
        that:
          - ( ansible_memory_mb.real.total | int ) >= ( min_mem_mb | int )
        msg:
          - "This server should have at least {{(min_mem_mb | int) / 1000}} GB of total memory."
          - "It only has {{( ansible_memory_mb.real.total | int) / 1000}} GB."
          - "Add --skip-tags skipmemfail to bypass"
      tags:
        - skipifbelowspecs
        - skipmemfail
    ## block end
    tags:
      - memory_check
      - detectableonly

####################################################################
## CPU Check
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml --tags cpu_check -e "use_pause=0" -e "min_cores_num=1"
#   make it fail
#     ansible-playbook viya_pre_install_playbook.yml --tags cpu_check -e "use_pause=0" -e "min_cores_num=100"

  - block:
    - name: Querying CPUs
      setup:
        filter: "ansible_processor*"
      register: processors
    - name: Show the processor information gathered by Ansible
      debug: var=processors.ansible_facts

    - name: Querying detailed CPU info from /proc/cpuinfo
      shell: "cat /proc/cpuinfo"
      changed_when: False
      check_mode: no
      register: detailedcpuinfo
    - name: Show the detailed cpu info from /proc/cpuinfo
      debug: var=detailedcpuinfo.stdout_lines

    - name: Querying number of CPUs from /proc/cpuinfo
      # commenting out because this does not work well on OpenStack
      #shell: "cat /proc/cpuinfo | grep 'cpu cores' | uniq | awk -F'[:]' '{print $2}' "
      # instead, I found this command which seems to work better. https://access.redhat.com/discussions/480953
      ## note that this does not seem to work on Oracle Linux. :-(
      shell: "egrep -e 'core id' -e ^physical /proc/cpuinfo|xargs -l2 echo|sort -u | wc -l"
      changed_when: False
      check_mode: no
      register: cpuinfo
    - name: Show the number of CPUs
      debug: var=cpuinfo.stdout

    - name: Assert that there are enough ({{min_cores_num}}) cores on the server
      assert:
        that:
          - (cpuinfo.stdout | int) >= (min_cores_num | int)
        msg:
          - "This server should have at least ({{min_cores_num}}) cores."
          - "It only has {{(cpuinfo.stdout | int)}} core(s)."
          - "Add --skip-tags skipcoresfail to bypass."
      tags:
        - skipifbelowspecs
        - skipcoresfail
    ## block end
    tags:
      - cpu_check
      - detectableonly

####################################################################
## Storage Check
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml --tags storage -e "use_pause=0" --extra-vars '{"storage_list": {  "path": "/" , "min_storage_mb": 1 , "min_io": 1 } }'
#   make it fail

  - block:
    - name: Query the storage for all servers being used in deployment
      setup:
        filter: "*mount*"
      register: mount
    - name: Show the mountpoint information gathered by Ansible
      debug: var=mount

    - name: "Make sure all directories exist, create them if needed"
      file:
        path: "{{ item.path }}"
        state: directory
      with_items: "{{storage_list}}"

    - name: "Get status for directories"
      stat:
        path: "{{ item.path }}"
      register: "directories_exists"
      with_items: "{{storage_list}}"

    - name: Show the status of directories
      debug: var=directories_exists

    - name: "Assert that directories exist, fail if they don't"
      assert:
        that:
          - item.stat.exists and (item.stat.isdir or item.stat.islnk)
        msg:
          - "The directory {{item.item.path}} does not exist on this server."
          - "Edit the storage_list variable if this is a mistake."
          - "Or add --skip-tags skipstoragefail to bypass"
      with_items:
        - "{{directories_exists.results}}"
      tags:
        - skipifbelowspecs
        - skipstoragefail

    - name: Get the amount of free disk space (in MB) of each directory.
      shell: df -Ph "{{ item.item.path }}" --block-size=M | tail -1 | awk '{print $4}' | sed 's/[^0-9]*\([0-9]\+\)[^0-9]*/\1/'
      changed_when: False
      check_mode: no
      with_items:
        - "{{directories_exists.results}}"
      register: df_result
    - name: Show the results of the df command
      debug: var=df_result

    - name: "Assert that directories have enough free space"
      assert:
        that:
          - (item.stdout | int) >= (item.item.item.min_storage_mb | int)
        msg:
        - "The directory {{item.item.item.path}} only has {{(item.stdout | int) / 1000}} GB of free space."
        - "It should have at least {{(item.item.item.min_storage_mb | int) / 1000}} GB."
        - "Edit storage_list if this is a mistake."
        - "Or add --skip-tags skipstoragefail to bypass"
      with_items:
        - "{{df_result.results}}"
      tags:
        - skipifbelowspecs
        - skipstoragefail

  ##
  ## We need smarter checks:
  ## that /opt/sas and / are not the same partition
  ##
  ## ...

    - name: Get the device associated with all the directories.
      shell: df "{{ item.path }}" | tail -1 | awk '{print $1}'
      changed_when: False
      check_mode: no
      register: device_list
      with_items: "{{storage_list}}"
      tags:
        - storagedev

    - name: Show the device information
      debug: var=device_list.results
      tags:
        - storagedev

    ## end of block
    tags:
      - storage_check
      - detectableonly

####################################################################
## Hostname Length Check
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags hostname_length_check -e use_pause=0 -e max_hostname_length=64
#   make it fail
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags hostname_length_check -e use_pause=0 -e max_hostname_length=2

  - block:
    ## hostname block
    - name: Capture the length of the hostname
      shell: "hostname  | wc -c"
      changed_when: False
      check_mode: no
      register: hostname_length

    - name: Show the hostname length
      debug: var=hostname_length.stdout
    - name: Show the maximum desired hostname length
      debug: var=max_hostname_length

    - name: "Assert that hostname is not too long (less than {{max_hostname_length}} characters"
      assert:
        that:
          - (hostname_length.stdout | int) <= (max_hostname_length | int)
        msg:
          - "The hostname is too long."
          - "Yours is {{hostname_length.stdout | int}} characters long. It should be {{max_hostname_length}} characters or less."
          - "Add --skip-tags skipnamelengthfail to bypass"
      tags:
        - skipnamelengthfail

      ## if hostname and hostname -f don't return the same thing, rabbit causes issues.
    - name: Get the value of hostname
      shell: "hostname"
      changed_when: False
      check_mode: no
      register: normal_hostname

    - name: Get the value of hostname -f
      shell: "hostname -f"
      changed_when: False
      check_mode: no
      register: full_hostname

    - name: "Assert that <hostname -f> contains at least one dot"
      assert:
        that:
          -  "'.' in (full_hostname.stdout)"
        msg:
          - "Your fully qualified hostname value ({{full_hostname.stdout}}) does not contain a dot."
          - "So, it does not really look fully qualified."
          - "This will likely make rabbitmq unhappy. (because it uses long-form names to communicate between members of the cluster)"
          - "Add --skip-tags skipnamematchfail to bypass"
      tags:
        - skipnamematchfail


    ## end of hostname block
    tags:
      - hostname_length_check
      - hostname
      - detectableonly

####################################################################
## YUM Cache Config
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags yum_cache_config -e use_pause=0
#   make it fail

  - block:
    - name: Show message telling you that you wanted YUM cache 'on'
      debug: msg="You want YUM cache enabled"

    - name: Getting the amount of free disk space (in MB) in the /var/cache/yum directory.
      shell: df -Ph /var/cache/yum  --block-size=M | tail -1 | awk '{print $4}' | sed 's/[^0-9]*\([0-9]\+\)[^0-9]*/\1/'
      changed_when: False
      check_mode: no
      register: var_cache_yum_actual

    - name: Show information about YUM cache storage
      debug: msg="You have {{var_cache_yum_actual.stdout}} MB free in /var/cache/yum and you need {{yum_cache_min_space_mb}} MB free"


    - name: "Assert that there is enough space for YUM cache"
      assert:
        that:
          - var_cache_yum_actual.stdout | int > yum_cache_min_space_mb | int
        msg:
          - "There is only {{var_cache_yum_actual.stdout | int}} MB available for YUM cache."
          - "You need at least {{yum_cache_min_space_mb}} MB"
          - "Either add more space or set the yum_cache_yn to 0."
          - "Add --skip-tags skipyumspacefail to bypass"
      tags:
        - skipifbelowspecs
        - skipyumspacefail

    when:  yum_cache_yn | bool  == true
    tags:
      - yumcache
      - detectableonly

  ##
  ## If the location of the YUM cache (/var/cache/yum) has the space for it (10-15 GB), set this to 1
  ## If not, leave this to zero
  ##
  - name: Enable/disable YUM caching ({{yum_cache_yn}})
    lineinfile:
      dest=/etc/yum.conf
      regexp="^keepcache="
      line="keepcache={{yum_cache_yn}}"
    tags:
      - yum_cache_config
      - fixable

####################################################################
## Required Packages Config
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags required_packages_config -e use_pause=0
#   make it fail
#     

  - block:
    ## block start

    ## works for either RHEL 6 or RHEL 7
    - name: Ensures required packages are present
      yum:
        name: "{{ item }}"
        state: present
      with_items:
        - "{{yum_packages_general}}"
      tags:
        - packages

    ## RHEL 6 specific
    - name: Ensures required RHEL6 packages are present
      yum:
        name: "{{ item }}"
        state: present
      with_items:
        - "{{yum_packages_rhel6}}"
      when: ansible_distribution_major_version == '6'
      tags:
        - packages

    ## RHEL 7 specific
    - name: Ensures required RHEL7 packages are present
      yum:
        name: "{{ item }}"
        state: present
      with_items:
        - "{{yum_packages_rhel7}}"
      when: ansible_distribution_major_version == '7'
      tags:
        - packages

    ##
    ## we need systemd to be above 219-30
    ## try to make sure we have the latest one.
    ##
    - name: Ensures systemd package is the most recent
      yum:
        name: systemd
        state: latest
      when: ansible_distribution_major_version == '7'
      tags:
        - packages

    - name: Ensures "nice to have" packages are present
      yum:
        name: "{{ item }}"
        state: present
      ignore_errors: yes
      with_items:
        - "{{yum_packages_nicetohave}}"
      tags:
        - packages
        - nicetohave

    ## On Linux 7.x, verify that the systemd package on each machine is at version 219-30 or later.
    - block:

      ##
      ## this code needs to be improved.
      ## it looks for version 219 only. lower or higher will fail.
      ##
      - name: obtain systemd version
        shell: systemctl --version | head -n 1
        register: systemd_version
        changed_when: False
        check_mode: no

      - name: Display systemd version
        debug: var=systemd_version
        #when: systemd_version.stdout != "systemd 219"

      - name: "Unexpected version of systemd: {{ systemd_version.stdout }}. Failing the playbook"
        fail:
        when: systemd_version.stdout != "systemd 219"

      when: ansible_distribution_major_version == '7'
      tags:
        - systemd
        - packages

    ## block end
    tags:
      - required_packages_config
      - fixable

####################################################################
## SSH MaxStartups Config
####################################################################
## read more about SSH maxstartups at:
## https://stackoverflow.com/questions/4812134/in-sshd-configuration-what-is-maxstartups-103060-means 

# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags ssh_maxstartups_config -e use_pause=0
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags ssh_maxstartups_config -e use_pause=0 -e maxstartups_val=500
#   make it fail
#

  - block:
    ## block start

    - name: Capture current SSH maxstartups value (third one)
      shell: "sshd -T | grep maxstartups | awk -F'[: ]' '{print $4}'"
      changed_when: False
      check_mode: no
      register: cur_maxstartups_3
      
    - name: Show the current value for sshd MaxStartups
      debug: var=cur_maxstartups_3.stdout
        
    ## case 1: current value is bigger, we leave as-is
    - block:
      - debug: msg="Your current maxstartups value ({{cur_maxstartups_3.stdout}}) is larger or equal than the desired maxstartups value ({{maxstartups_val}}) \n We won't udpate the maxstartups value"
      when: cur_maxstartups_3 is defined and ((cur_maxstartups_3.stdout | int) > maxstartups_val|int)

    ## case 2: current value is smaller, we update
    - block:
      - debug: msg="your current maxstartups value ({{cur_maxstartups_3.stdout}}) is smaller than the desired maxstartups value ({{maxstartups_val}})"
      
      - name: Capture current SSH maxstartups value (first one)
        shell: "sshd -T | grep maxstartups | awk -F'[: ]' '{print $2}'"
        changed_when: False
        check_mode: no
        register: cur_maxstartups_1
        
      - name: Capture current SSH maxstartups value (second one)
        shell: "sshd -T | grep maxstartups | awk -F'[: ]' '{print $3}'"
        changed_when: False
        check_mode: no
        register: cur_maxstartups_2

      - set_fact: 
          newsshline: "{% if cur_maxstartups_1.stdout=='' or cur_maxstartups_2.stdout=='' %}MaxStartups {{maxstartups_val}}{% else %}MaxStartups {{cur_maxstartups_1.stdout}}:{{cur_maxstartups_2.stdout}}:{{maxstartups_val}}{% endif %}" 

      - debug: var=newsshline

      - name: Configure SSH with MaxStartups={{maxstartups_val}} in /etc/ssh/sshd_config
        lineinfile:
          dest=/etc/ssh/sshd_config
          regexp="^MaxStartups"
          line="{{newsshline}}"
          state=present
          backup=yes
        register: sshd_config
        
      - debug: var=sshd_config

      ##
      ## Re-run sshd -T to see if it executes successfully or complains,
      ## in order to validate our change to sshd_config file did not break sshd settings
      ##
      - name: Run sshd -T to test for validity of /etc/ssh/sshd_config file after making our change
        shell: "sshd -T"
        changed_when: False
        check_mode: no
        register: sshd_validation_results
        ignore_errors: true
        when: sshd_config is defined

      - name: Show results of sshd -T execution (return code)
        debug:
          msg:
            #- "{{sshd_validation_results.stdout_lines}}"
            - "{{sshd_validation_results.rc}}"
        when: sshd_validation_results.rc is defined

      - block:
        - name: We restart the SSHD service
          service:
            name: sshd
            state: restarted
          when: sshd_config.changed
        when: sshd_validation_results.rc is defined and sshd_validation_results.rc == 0

      - name: Clarifying message in case of issue 
        assert:
          that:
            - sshd_validation_results.rc is defined and sshd_validation_results.rc == 0
          msg:
            - "It seems that something went wrong with the file /etc/ssh/sshd_config"
            - "You should review the MaxStartups line, and correct it"
            - "Do NOT restart the sshd service until the command 'sshd -T' returns a clean output"
        when: sshd_validation_results.rc is defined 
        
      when: cur_maxstartups_3 is defined and ((cur_maxstartups_3.stdout | int) < maxstartups_val|int)

    ## block end
    tags:
      - ssh_maxstartups_config
      - fixable

####################################################################
## User and Group Config
####################################################################
# Test harness
# this should fail quick because group name is already in use
#  ansible-playbook viya_pre_install_playbook.yml --tags user_and_group_config -e "use_pause=0" --extra-vars '{"custom_group_list": {  "group": "root" , "gid":"2500"  }     }'
#  ansible-playbook viya_pre_install_playbook.yml --tags user_and_group_config -e "use_pause=0" --extra-vars '{"custom_group_list": {  "group": "weirdname" , "gid":"0"  }     }'
#  ansible-playbook viya_pre_install_playbook.yml --tags user_and_group_config -e "use_pause=0" --extra-vars '{"custom_group_list": {  "group": "sas" , "gid":"1001"  }     }'

  - block:
    ## block start

    ## Check if the groups exist, and if so, what its gid is
    - name: Get the GID each group name
      shell:  "getent group {{item.group}} | awk -F'[:]' '{print $3}' "
      changed_when: False
      check_mode: no
      register: gather_gid
      failed_when: no
      with_items:
        - "{{ custom_group_list }}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - group
    - name: display the group
      debug: var=gather_gid
      when: create_local_cas_and_sas_accounts == true
      tags:
        - group

    - name: Assert that the groups either does not exist, or if it does, has the desired GID
      any_errors_fatal: true
      assert:
        that:
          - ( item.stdout == "" ) or (( item.stdout | int ) == ( item.item.gid | int))
        msg:
          - "We looked for the group '{{item.item.group}}'. (getent group {{item.item.group}})"
          - "We found that this group already exists and has a GID of {{item.stdout}}"
          - "We were expecting a GID of {{item.item.gid}}"
          - "Review the 'custom_group_list' variable."
      with_items:
        - "{{gather_gid.results}}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - group

    ##
    ## if we passed the previous check, we know that either:
    ##
    ##    * the group exists and has the right GID, or
    ##    * the group does not exist at all
    ##

    ## Check if the groups exist, and if so, what its gid is
    - name: Get the group from the GID
      shell:  "getent group {{item.gid}} | awk -F'[:]' '{print $1}' "
      changed_when: False
      check_mode: no
      register: gather_group
      failed_when: no
      with_items:
        - "{{ custom_group_list }}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - group
    - name: display the group
      debug: var=gather_group
      when: create_local_cas_and_sas_accounts == true
      tags:
        - group

    - name: Assert that the either the GID is not used, or if used, it's used for the right group
      any_errors_fatal: true
      assert:
        that:
          - ( item.stdout == "" ) or (( item.stdout ) == ( item.item.group ))
        msg:
          - "We looked for the GID '{{item.item.gid}}'. (getent group {{item.item.gid}})"
          - "We found that this GID is already assigned but to the group '{{item.stdout}}'"
          - "We were expecting a group of '{{item.item.group}}'"
          - "Review the 'custom_group_list' variable."
      with_items:
        - "{{gather_group.results}}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - group

    ##
    ## if we got this far, it's safe to create/update the group.
    ## let's create the group
    ##
    - name: Ensure groups are present
      any_errors_fatal: true
      group:
        name: "{{ item.group }}"
        gid: "{{item.gid}}"
        state: present
      with_items:
        - "{{ custom_group_list }}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - group

    ##
    ## now we deal with the users.
    ## make sure the user either does not exist, or if it does, it has the right UID.
    ##
    - name: Get the UID for each user
      shell:  "getent passwd {{item.name}} | awk -F'[:]' '{print $3}' "
      changed_when: False
      check_mode: no
      register: gather_uid
      failed_when: no
      with_items:
        - "{{ custom_user_list }}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - users
    - name: display the uid
      debug: var=gather_uid
      when: create_local_cas_and_sas_accounts == true
      tags:
        - users

    - name: Assert that the user either does not exist, or if it does, has the desired UID
      any_errors_fatal: true
      assert:
        that:
          - ( item.stdout == "" ) or (( item.stdout | int ) == ( item.item.uid | int))
        msg:
          - "We looked for the user '{{item.item.name}}'. (getent passwd {{item.item.name}})"
          - "We found that this user already exists and has a UID of {{item.stdout}}"
          - "We were expecting a UID of {{item.item.uid}}"
          - "Review the 'custom_user_list' variable."
      with_items:
        - "{{gather_uid.results}}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - users

    ## Check if the user exist, and if so, what its gid is
    - name: Get the user name from the wanted UIDs
      shell:  "getent passwd {{item.uid}} | awk -F'[:]' '{print $1}' "
      changed_when: False
      check_mode: no
      register: gather_name
      failed_when: no
      with_items:
        - "{{ custom_user_list }}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - users
    - name: display the user names
      debug: var=gather_name
      when: create_local_cas_and_sas_accounts == true
      tags:
        - users

    - name: Assert that the either the UID is not used, or if used, it's used for the right user name
      any_errors_fatal: true
      assert:
        that:
          - ( item.stdout == "" ) or (( item.stdout ) == ( item.item.name ))
        msg:
          - "We looked for the UID '{{item.item.uid}}'. (getent group {{item.item.uid}})"
          - "We found that this UID is already assigned but to the user '{{item.stdout}}'"
          - "We were expecting a user of '{{item.item.name}}'"
          - "Review the 'custom_user_list' variable."
      with_items:
        - "{{gather_name.results}}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - users


    - name: Ensure users are present
      user:
        name: "{{ item.name }}"
        uid: "{{item.uid}}"
        group: "{{ item.group }}"
        groups: "{{ item.groups }}"
        append: yes
        state: present
      with_items:
         - "{{ custom_user_list }}"
      when: create_local_cas_and_sas_accounts == true
      tags:
        - users

    - name: Ensure umask are correctly set for users
      lineinfile:
        dest: "/home/{{ item[0].name }}/.bashrc"
        line: "{{ item[1] }}"
        state: present
        create: yes
      with_nested:
         - "{{ custom_user_list }}"
         - [ "# Set umask to 0002 instead of the default 0022, so that the default permissions for files", "# created by this user grant write permission to members of this user's primary group", "umask {{umask_value}}" ]
      when: create_local_cas_and_sas_accounts == true
      tags:
        - users

    ## block end
    tags:
      - user_and_group_config

####################################################################
## SELINUX Config
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml --tags selinux_config -e use_pause=0
#   make it fail
#

  - block:
    ## block start

    - name: Disable SELinux (will only take effect after a reboot)
      ## this could also be set to permissive
      lineinfile:
        dest=/etc/selinux/config
        regexp="^SELINUX=.*"
        line="SELINUX=permissive"
        state=present
        backup=yes
      tags:
        - prereq
        - selinux

    - name: Get selinux status
      shell: /usr/sbin/getenforce
      changed_when: False
      check_mode: no
      register: selinuxstatus
      tags:
        - prereq
        - selinux

    - name: Make SELinux permissive (in the current session) but only if it's currently enabled
      shell: "setenforce 0"
      when: selinuxstatus.stdout is defined and  selinuxstatus.stdout == 'Enforcing'
      ignore_errors: yes
 
    ## block end
    tags:
      - prereq
      - selinux_config

####################################################################
## Ulimit Config
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml --tags ulimit_config -e use_pause=0
#   make it fail
#

  - block:
    ## block start

    ## only do this if the CAS user exist. If not, skip the whole thing.
    - name: check if the cas account exitst
      shell:  "getent passwd {{cas_user}} > /dev/null "
      changed_when: False
      check_mode: no
      register: check_cas_user
      failed_when: false
      tags:
        - casuser

    - name: Show whether the cas account exists or not
      debug: var=check_cas_user
      tags:
        - casuser

    - name: Inform user of skipped ulimits checks.
      debug:
        msg:
          - "the cas user ({{cas_user}}) does not seem to exist."
          - "that user would be used to check ulimit values. "
          - "this check will be skipped until {{cas_user}} exists"
      when: check_cas_user.rc | int != 0
      tags:
        - casuser

    - block:

      ## gather ulimit values - needs to be done as a regular user (cas)
      - name: Gather the current Ulimit values (soft nofile) for user "{{cas_user}}"
        become: yes
        become_user: "{{cas_user}}"
        shell: ulimit -n -S
        changed_when: False
        check_mode: no
        register: ulimit_soft_nofile

      - name: Gather the current Ulimit values (hard nofile) for user "{{cas_user}}"
        become: yes
        become_user: "{{cas_user}}"
        shell: ulimit -n -H
        changed_when: False
        check_mode: no
        register: ulimit_hard_nofile

      - name: Gather the current Ulimit values (soft nproc) for user "{{cas_user}}"
        become: yes
        become_user: "{{cas_user}}"
        shell: ulimit -u -S
        changed_when: False
        check_mode: no
        register: ulimit_soft_nproc

      ## display values
      - name: Show the value of ulimit_soft_nofile
        debug: var=ulimit_soft_nofile.stdout
      - name: Show the value of ulimit_hard_nofile
        debug: var=ulimit_hard_nofile.stdout
      - name: Show the value of ulimit_soft_nproc
        debug: var=ulimit_soft_nproc.stdout

      ## no updates needed nofile ulimit
      - block:
        - name: Show the message when no update is needed for nofile ulimit
          debug: msg="Your current nofile ulimit value ({{ulimit_soft_nofile.stdout}}(soft)/{{ulimit_hard_nofile.stdout}}(hard)) is greater or equal than desired value ({{ulimit_nofile_val}}) \n There is no need to udpate it"
        when: ulimit_soft_nofile is defined and (ulimit_soft_nofile.stdout | int >= ulimit_nofile_val | int or ulimit_hard_nofile.stdout | int >= ulimit_nofile_val | int )

      ## update nofile ulimit
      - block:
        - name: Show the message when updates are needed for nofile ulimit
          debug: msg="Your current nofile ulimit value ({{ulimit_soft_nofile.stdout}}(soft)/{{ulimit_hard_nofile.stdout}}(hard)) is less than desired value ({{ulimit_nofile_val}}) \n We will update the value"
        - name: Adjust limits in /etc/security/limits.d/va81.conf
          blockinfile:
            dest: /etc/security/limits.d/va81.conf
            backup: yes
            create: yes
            owner: root
            group: root
            mode: 0644
            marker: "#### -- {mark} VDMML 8.1 prereqs - ulimits - nofile"
            insertafter: EOF
            block: |
              @{{sas_group}}       -    nofile    {{ulimit_nofile_val}}
        when: ulimit_soft_nofile is defined and (ulimit_soft_nofile.stdout | int < ulimit_nofile_val |int or ulimit_hard_nofile.stdout | int < ulimit_nofile_val | int)

      ## no need to update nproc ulimits
      - block:
        - name: Show the message when no updates are needed for nproc ulimit
          debug: msg="Your current nproc ulimit value ({{ulimit_soft_nproc.stdout}}) is greater or equal than desired value ({{ulimit_soft_nproc_val}}) \n There is no need to udpate it"
        when: ulimit_soft_nproc is defined and ( (ulimit_soft_nproc.stdout | int) >= ulimit_soft_nproc_val | int)

      ## update nproc
      - block:
        - name: Show the message when updates are needed for nproc ulimit
          debug: msg="Your current nproc ulimit value ({{ulimit_soft_nproc.stdout}}) is less than desired value ({{ulimit_soft_nproc_val}}) \n We will update the value"
        - name: Adjust limits in /etc/security/limits.d/va81.conf
          blockinfile:
            dest: /etc/security/limits.d/va81.conf
            backup: yes
            create: yes
            owner: root
            group: root
            mode: 0644
            marker: "#### -- {mark} VDMML 8.1 prereqs - ulimits - nproc"
            insertafter: EOF
            block: |
              @{{sas_group}}       soft  nproc  {{ulimit_soft_nproc_val}}
              @{{sas_group}}       hard  nproc  {{ulimit_soft_nproc_val}}
        when: ulimit_soft_nproc is defined and ((ulimit_soft_nproc.stdout | int) < ulimit_soft_nproc_val | int)

      when: (check_cas_user.rc | int) == 0
      tags:
        - ulimit

    ## block end
    tags:
      - ulimit_config

####################################################################
## OS-level Firewall Config
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags network_and_bandwidth_check -e use_pause=0
#   make it fail
#

  - block:
    ## block start

    ## on RHEL 7, we make sure that firewalld is disabled and stopped
    - name: Check if firewalld exists
      stat: path=/usr/lib/systemd/system/firewalld.service
      register: firewall_status
      tags:
        - prereq
        - firewall

    - name: Disable firewalld (on RHEL 7)
      service:
        name=firewalld
        enabled=no
        state=stopped
      when: ansible_distribution_major_version == '7' and firewall_status.stat.exists
      tags:
        - prereq
        - firewall

    ## on RHEL 6, we deal with iptables and iptables6
    - name: Disable iptables (on RHEL 6)
      service:
        name=iptables
        enabled=no
        state=stopped
      when: ansible_distribution_major_version == '6'
      ignore_errors: yes
      tags:
        - prereq
        - firewall

    - name: Disable ip6tables (on RHEL 6)
      service:
        name=ip6tables
        enabled=no
        state=stopped
      when: ansible_distribution_major_version == '6'
      ignore_errors: yes

     ## block end
    tags:
      - prereq
      - os_firewall_config

####################################################################
## NTP Check
####################################################################
# Test harness:
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml -i inventory --tags ntp_config -e use_pause=0
#   make it fail
#

  - block:
    ## block start

    - name: Check if ntp service exists
      stat: path=/usr/lib/systemd/system/ntpd.service
      register: ntpd_service

    - name: Ensures NTP service is started and enabled
      service:
        name: "{{ item }}"
        state: started
        enabled: yes
      when: ntpd_service.stat.exists
      with_items:
        - ntpd

    ## block end
    tags:
      - fixable
      - ntp_config

####################################################################
## Network and Bandwidth Check
####################################################################
# Test harness
#   make it pass
#     ansible-playbook viya_pre_install_playbook.yml --tags network_and_bandwidth_check -e "use_pause=0" -e "max_num_nics=10"
#   make it fail
#     ansible-playbook viya_pre_install_playbook.yml --tags network_and_bandwidth_check -e "use_pause=0" -e "max_num_nics=0"

  - block:
    ## block start

    - name: "Count the number of NICs (other than loopback)"
      shell: "ls -A /sys/class/net | grep -v lo | wc -l"
      changed_when: False
      check_mode: no
      register: numnics
      tags:
        - nics

    - name: Show the number of NICs found on the server
      debug: var=numnics.stdout
      tags:
        - nics

    - name: Assert that there only ({{max_num_nics|int}}) NICs or less on the server
      assert:
        that:
          - (numnics.stdout | int) <= (max_num_nics | int)
        msg:
          - "This server seems to have {{numnics.stdout|int}} Network Interfaces."
          - "It should only have {{max_num_nics|int}}."
          - "This will require more work to successfully deploy consul."
          - "Please read the following SAS Note for details on how to deal with it :<todo>"
          - "Add --skip-tags skipnicssfail to suppress this message."
      tags:
        - nics
        - skipifbelowspecs
        - skipnicssfail

    - name: "Trying to hit the SAS yum repository "
      shell: "curl -v -k https://{{item}}/"
      changed_when: False
      check_mode: no
      register: curl_sas_yum
      with_items: "{{sas_yum_urls}}"
      failed_when: "('Connect' in curl_sas_yum.stderr and item in curl_sas_yum.stderr ) != true"
      tags:
        - yum_repo

    ## Generic Speed Test of Connection to the Internet
    - name: Bandwidth to the Internet Check - Download 100 MB file
            - (the higher the better, otherwise downloading rpms will take long)
      shell: "time curl -o /dev/null  http://speedtest.sea01.softlayer.com/downloads/test100.zip 2>&1 | tail"
      check_mode: no
      changed_when: False
      register: time_speed_test
      tags:
        - bandwidth

    - name: Display bandwidth test results
      debug: var=time_speed_test.stdout_lines
      tags:
        - bandwidth

    ## DNS and Addressing
    - name: Hostname Ping check - (a server has to know itself by its hostname)
      shell: 'ping `hostname` -c 1'
      check_mode: no
      changed_when: False
      register: hostname_ping
      tags:
        - hostname

    - name: Show the results of the self ping test
      debug: var=hostname_ping.stdout_lines
      tags:
        - hostname

    ## block end
    tags:
      - network_and_bandwidth_check
      - detectableonly
 
 
  ## DNS and Addressing
  - name: Various Hostname check
          - (hostname ; hostname -a ; hostname -A ; hostname -f ; hostname -i ; hostname -I)
    shell: "hostname ; hostname -a ; hostname -A ; hostname -f ; hostname -i ; hostname -I"
    check_mode: no
    changed_when: False
    register: various_hostname_test
    ignore_errors: yes
    tags:
      - hostname

  - name: Show the result of 'hostname ; hostname -a ; hostname -A ; hostname -f ; hostname -i ; hostname -I'
    debug: var=various_hostname_test.stdout_lines
    tags:
      - hostname
      - extras

####################################################################
## Hostname checks
####################################################################
# Test harness

  ## Check /etc/hosts
  - name: Check /etc/hosts
    shell: "cat /etc/hosts"
    check_mode: no
    changed_when: False
    register: etc_hosts
    ignore_errors: yes
    tags:
      - hosts

  - name: Show the content of /etc/hosts
    debug: var=etc_hosts.stdout_lines
    tags:
      - hosts
      - extras

####################################################################
## Service Command Check
####################################################################
# Test harness

  - name: check if the {{sas_user}} account exitst
    shell:  "getent passwd {{sas_user}} > /dev/null "
    changed_when: False
    check_mode: no
    register: check_sas_user
    ignore_errors: yes
    tags:
      - service_command
  - name: Show whether user 'sas' exists
    debug: var=check_sas_user
    tags:
      - service_command

  - name: Inform user of skipped check.
    debug:
      msg:
        - "the sas user ({{sas_user}}) does not seem to exist."
        - "we therefore can't access if it can use the service command "
        - "this check will be skipped until {{sas_user}} exists"
    when: check_sas_user.rc | int != 0
    tags:
      - service_command

  - name: Check if {{sas_user}} has access to service command
    shell: "which service"
    become: yes
    become_user: "{{sas_user}}"
    check_mode: no
    changed_when: False
    register: service_command
    failed_when: false
    when: check_sas_user.rc | int == 0
    tags:
      - service_command

  - name: Assert that {{sas_user}} can use service
    assert:
      that:
        - service_command.rc != 1
      msg:
        - "Apparently, {{sas_user}} can't use the service command"
        - "the result of 'which service' is:"
        - "      {{service_command.stderr}}"
    when: check_sas_user.rc | int == 0
    tags:
      - service_command

####################################################################
## e-mailing the results (optional)
####################################################################

  - name: Sending the log file by e-mail
    check_mode: no
    local_action: mail
      host="some_smtp_host"
      port=25
      subject="Ansible log file"
      body="Hello, \n please find the Ansible log file attached. \n \n Sincerely,  Ansible. "
      from="server@localhost"
      to="{{admin_email}}"
      attach="./deployment.log"
    when: admin_email is defined
    ignore_errors: yes
    tags:
      - mail
